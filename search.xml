<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据怎样才算真正落盘]]></title>
    <url>%2F2019%2F03%2F%E6%95%B0%E6%8D%AE%E6%80%8E%E6%A0%B7%E6%89%8D%E7%AE%97%E7%9C%9F%E6%AD%A3%E8%90%BD%E7%9B%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>体系结构</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>x86</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[纠正几个Linux的IO监控指标的误区]]></title>
    <url>%2F2019%2F03%2F%E7%BA%A0%E6%AD%A3%E5%87%A0%E4%B8%AALinux%E7%9A%84IO%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E7%9A%84%E8%AF%AF%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[前言本文主要面向初级工程师，假设你已经对的Linux性能监控工具有所了解，例如top、vmstat、mpstat、iostat等。 日常工作中，碰到IO性能问题是常有的事，本文主要探讨Linux下的IO监控里容易产生误区的点。 这些误区，一方面是因为工程师想当然，不仔细看文档，不理解底层细节。另一方面是因为Linux内核和监控工具的发展，落后于存储设备硬件的发展，软硬件发展存在脱节，因此监控工具不能反映硬件的真实情况。 纠正这些误区，对这些监控和背后原理理解越深刻，越能评估一个系统性能的真实情况，定位和解决问题也更容易。 环境12345678910Summary: ASRock Z370M-ITX/ac, 1 x Core i5-8400 2.80GHz, 15.2GB / 16GB 2400MT/s DDR4System: ASRock Z370M-ITX/acProcessors: 1 x Core i5-8400 2.80GHz 100MHz FSB (6 cores)Memory: 15.2GB / 16GB 2400MT/s DDR4 == 2 x 8GBDisk: sda (scsi0): 240GB (0%) JBOD == 1 x LITEON-EGT-240N9SDisk: sdb: 100GB JBOD == 1 x INTEL-SSDSA2BZ100G3Disk: sdc (scsi2): 1.0TB JBOD == 1 x WDC-WD10JPLX-00MBPT1OS: CentOS Linux 7.6.1810 (Core) , Linux 3.10.0-957.10.1.el7.x86_64 x86_64, 64-bitBIOS: AMI P1.50 11/16/2017Hostname: MiWiFi-R1CM-srv 这是一台普通的x86台式机，sdb是Intel SSD 710 Series，sdc是西数的7200转黑盘。使用这两块盘来做后面的实验。 正文下面介绍四个容易产生误区的指标，分别是%iowait、await、svctm、%util。 %iowait 正确：%iowait（wa）是CPU的一种IDLE，%iowait大小不能用来反映IO瓶颈。 误区：（1）%iowait高，CPU很忙。（2）%iowait高，磁盘IO瓶颈。 解释%iowait（来自mpstat）和wa（来自top、vmstat）是同一个指标，表示在一个采样周期内，CPU空闲在等待未完成的IO请求所占的白分比。iowait的产生要满足两个条件，一是进程在等io，二是等io时没有进程可运行。 误区（1）%iowait高，CPU很忙。先看官方文档说明：123456man vmstatwa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.man mpstat%iowaitShow the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request. 文档已经明确显示，%iowait是一种IDLE，尤其在2.5.41以前的旧内核里，%iowait是直接统计在idle里的。既然%iowait的显示的百分比是一种IDLE，它是可以被其它进程使用的。%iowait高，CPU其实很闲，因为其它进程都在随眠。 实验验证12345678910111213141516171819202122232425262728293031323334353637383940414243在这台完全空闲的机器上，对sdc进行随机读，可以看到iops大约是60多，这与7200rpm的机械硬盘性能是相符的。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdc -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -name=rand_100read_4krand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1fio-3.1Starting 1 threadJobs: 1 (f=1): [r(1)][6.7%][r=252KiB/s,w=0KiB/s][r=63,w=0 IOPS][eta 02m:48s]此时，我们看%iowait，可以看到在核心6上，100%的iowait。top - 22:27:51 up 3:08, 3 users, load average: 2.37, 2.40, 1.31Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 0.0 us, 0.3 sy, 0.0 ni, 0.0 id, 99.7 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 15934084 total, 11211948 free, 290604 used, 4431532 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14941848 avail Mem 这时，我们起一个stress打满CPU看看。[root@MiWiFi-R1CM-srv ~]# stress -c 6stress: info: [9856] dispatching hogs: 6 cpu, 0 io, 0 vm, 0 hdd可以看到，起了6个stress打满了6个核心，fio进程还在运行，但是%iowait已经被“吃掉”了，说明这部分IDLE的CPU时间片已经被stress征用了。top - 22:28:36 up 3:09, 3 users, load average: 3.63, 2.67, 1.45Tasks: 162 total, 7 running, 155 sleeping, 0 stopped, 0 zombie%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 15934084 total, 11208508 free, 293992 used, 4431584 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14938420 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9857 root 20 0 7308 96 0 R 100.0 0.0 0:22.69 stress 9861 root 20 0 7308 96 0 R 100.0 0.0 0:22.71 stress 9862 root 20 0 7308 96 0 R 100.0 0.0 0:22.71 stress 9858 root 20 0 7308 96 0 R 99.7 0.0 0:22.67 stress 9859 root 20 0 7308 96 0 R 99.7 0.0 0:22.68 stress 9860 root 20 0 7308 96 0 R 99.7 0.0 0:22.70 stress 9834 root 20 0 1091604 293556 261636 S 0.3 1.8 0:01.06 fio 误区（2）%iowait高，磁盘IO瓶颈。由之前的解释可以看到，这个指标是描述CPU的IDLE的，它与磁盘IO是否到达瓶颈一点关系都没有，它都不是一个用来描述磁盘IO性能的指标。 实验验证总结%iowait大小受CPU的负载变化而变化，这个指标如果很大，除了能说明CPU很闲在等待IO以外，并不能反映IO压力或者瓶颈。分析IO瓶颈，关键是去看Linux的IO的监控指标是否打满了存储设备的性能，因此必须对存储设备在当前IO模型下具备的真实性能有所了解。 原理可以参考这个文章：朱辉(茶水)： Linux Kernel iowait 时间的代码原理 await 正确：表示IO从创建到完成的平均时间，包含IO排队时间+存储设备处理时间。 误区：（1）混淆%iowait和await。（2）认为await高等于磁盘慢。 解释svctm 正确：iostat里的svctm并不能准确反映存储设备的处理时间。 误区：svctm就是存储设备硬件处理IO的时间。 解释 %util 正确：iostat里的%util并不能准确反映存储设备负载。 误区：%util就是存储设备硬件的使用率，和CPU使用率一样。 解释]]></content>
      <categories>
        <category>性能分析</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>性能监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2019%2F03%2F%E4%BD%A0%E5%A5%BD%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
