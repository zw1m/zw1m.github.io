<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用wireguard打通内网机器和外网云服务器]]></title>
    <url>%2F2019%2F03%2F%E4%BD%BF%E7%94%A8wireguard%E6%89%93%E9%80%9A%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8%E5%92%8C%E5%A4%96%E7%BD%91%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[场景这是我个人的需求，但是也有借鉴意义。需要在外部连接家里的机器做测试，家里路由器拨号之后没有公网IP，所以使用一台具有公网IP的阿里云ECS服务器作为中转，打通内网的机器和阿里云服务器。在以往，我会使用strongswan来搭建IKEv2的VPN来实现这个需求，但是现在有更简单好用的wireguard了。 wireguard介绍毕竟是VPN软件，所以需要翻墙访问。wireguard官网wireguard官网的视频介绍 环境两台CentOS 7.6，都可以访问公网。A机器仅有内网IP，B机器有公网IP和内网IP。 配置方式1234567891011121314151617181920212223242526272829303132333435A机器(家里)内网(192.168.31.0/24)B机器(ECS)内网(172.16.100.0/24)分别在每个机器执行:yum install -y epel-releasecurl -Lo /etc/yum.repos.d/wireguard.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repoyum install wireguard-dkms wireguard-tools -ymkdir -p /etc/wireguardumask 077cd /etc/wireguardwg genkey | tee privatekey | wg pubkey &gt; publickeyvim wg0.conf 内容如下:[Interface]PrivateKey = 本机的privatekeyListenPort = 2503PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADEPostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADEMTU = 1500[Peer]PublicKey = 远端的publickeyEndpoint = 远端的ip:2503（公网ip，至少需要一端有，没有就不写）AllowedIPs = 远端的网段启动systemctl start wg-quick@wg0.service echo &apos;net.ipv4.ip_forward = 1&apos; &gt;&gt; /etc/sysctl.conf 启用转发sysctl -p 分别A,B机器启动, 然后ping对方, 谁先ping, 谁就发起连接。由于我的环境里，只有阿里云ECS才有公网IP，所以我只能由家里机器发起ping。配置上开机启动服务。systemctl enable wg-quick@wg0.service 要开机自动连接到阿里云，就写个crontab的ping，每分钟ping一次好了。这里我使用家里的机器，ping阿里云服务器的内网IP，即可自动建立连接。123456789101112131415161718[root@MiWiFi-R1CM-srv ~]# cat /etc/crontab SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed* * * * * root ping -c1 172.16.100.38 &amp;&gt; /dev/nullwg show 查看状态 由于wireguard是内核模块，需要注意内核各个rpm包小版本一致。例如我这里，都是3.10.0-957.5.1.el7.x86_64这个小版本。12345678[root@MiWiFi-R1CM-srv ~]# rpm -qa | grep kernelkernel-3.10.0-957.5.1.el7.x86_64kernel-headers-3.10.0-957.5.1.el7.x86_64kernel-tools-libs-3.10.0-957.5.1.el7.x86_64kernel-devel-3.10.0-957.5.1.el7.x86_64kernel-debuginfo-3.10.0-957.5.1.el7.x86_64kernel-tools-3.10.0-957.5.1.el7.x86_64kernel-debuginfo-common-x86_64-3.10.0-957.5.1.el7.x86_64]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>VPN</tag>
        <tag>wireguard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据怎样才算真正落盘]]></title>
    <url>%2F2019%2F03%2F%E6%95%B0%E6%8D%AE%E6%80%8E%E6%A0%B7%E6%89%8D%E7%AE%97%E7%9C%9F%E6%AD%A3%E8%90%BD%E7%9B%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>体系结构</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>x86</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[纠正几个Linux的IO监控指标的误区]]></title>
    <url>%2F2019%2F03%2F%E7%BA%A0%E6%AD%A3%E5%87%A0%E4%B8%AALinux%E7%9A%84IO%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E7%9A%84%E8%AF%AF%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[前言本文主要面向初级工程师，假设你已经对的Linux性能监控工具有所了解，例如top、vmstat、mpstat、iostat等。 日常工作中，碰到IO性能问题是常有的事，本文主要探讨Linux下的IO监控里容易产生误区的点。 这些误区，一方面是因为工程师想当然，不仔细看文档，不理解底层细节。另一方面是因为Linux内核和监控工具的发展，落后于存储设备硬件的发展，软硬件发展存在脱节，因此监控工具不能反映硬件的真实情况。 纠正这些误区，对这些监控和背后原理理解越深刻，越能评估一个系统性能的真实情况，定位和解决问题也更容易。 环境12345678910Summary: ASRock Z370M-ITX/ac, 1 x Core i5-8400 2.80GHz, 15.2GB / 16GB 2400MT/s DDR4System: ASRock Z370M-ITX/acProcessors: 1 x Core i5-8400 2.80GHz 100MHz FSB (6 cores)Memory: 15.2GB / 16GB 2400MT/s DDR4 == 2 x 8GBDisk: sda (scsi0): 240GB (0%) JBOD == 1 x LITEON-EGT-240N9SDisk: sdb: 100GB JBOD == 1 x INTEL-SSDSA2BZ100G3Disk: sdc (scsi2): 1.0TB JBOD == 1 x WDC-WD10JPLX-00MBPT1OS: CentOS Linux 7.6.1810 (Core) , Linux 3.10.0-957.10.1.el7.x86_64 x86_64, 64-bitBIOS: AMI P1.50 11/16/2017Hostname: MiWiFi-R1CM-srv 这是一台普通的x86台式机，sdb是Intel SSD 710 Series，sdc是西数的7200转黑盘。使用这两块盘来做后面的实验。 正文下面介绍四个容易产生误区的指标，分别是%iowait、await、svctm、%util。顺序上会把%util放在svctm之前介绍。 %iowait 正确：%iowait（wa）是CPU的一种IDLE，%iowait大小不能用来反映IO瓶颈。 误区：（1）%iowait高，CPU很忙。（2）%iowait高，磁盘IO瓶颈。 解释%iowait（来自mpstat）和wa（来自top、vmstat）是同一个指标，表示在一个采样周期内，CPU空闲在等待未完成的IO请求所占的白分比。iowait的产生要满足两个条件，一是进程在等io，二是等io时没有进程可运行。 误区（1）%iowait高，CPU很忙。先看官方文档说明：123456man vmstatwa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.man mpstat%iowaitShow the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request. 文档已经明确显示，%iowait是一种IDLE，尤其在2.5.41以前的旧内核里，%iowait是直接统计在idle里的。既然%iowait的显示的百分比是一种IDLE，它是可以被其它进程使用的。%iowait高，CPU其实很闲，因为其它进程都在随眠。 实验验证12345678910111213141516171819202122232425262728293031323334353637383940414243在这台完全空闲的机器上，对sdc进行随机读，可以看到IOPS大约是60多，这与7200rpm的机械硬盘性能是相符的。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdc -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -name=rand_100read_4krand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1fio-3.1Starting 1 threadJobs: 1 (f=1): [r(1)][6.7%][r=252KiB/s,w=0KiB/s][r=63,w=0 IOPS][eta 02m:48s]此时，我们看%iowait，可以看到在核心6上，100%的iowait。top - 22:27:51 up 3:08, 3 users, load average: 2.37, 2.40, 1.31Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 0.0 us, 0.3 sy, 0.0 ni, 0.0 id, 99.7 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 15934084 total, 11211948 free, 290604 used, 4431532 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14941848 avail Mem 这时，我们起一个stress打满CPU看看。[root@MiWiFi-R1CM-srv ~]# stress -c 6stress: info: [9856] dispatching hogs: 6 cpu, 0 io, 0 vm, 0 hdd可以看到，起了6个stress打满了6个核心，fio进程还在运行，但是%iowait已经被“吃掉”了，说明这部分IDLE的CPU时间片已经被stress征用了。top - 22:28:36 up 3:09, 3 users, load average: 3.63, 2.67, 1.45Tasks: 162 total, 7 running, 155 sleeping, 0 stopped, 0 zombie%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 15934084 total, 11208508 free, 293992 used, 4431584 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14938420 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9857 root 20 0 7308 96 0 R 100.0 0.0 0:22.69 stress 9861 root 20 0 7308 96 0 R 100.0 0.0 0:22.71 stress 9862 root 20 0 7308 96 0 R 100.0 0.0 0:22.71 stress 9858 root 20 0 7308 96 0 R 99.7 0.0 0:22.67 stress 9859 root 20 0 7308 96 0 R 99.7 0.0 0:22.68 stress 9860 root 20 0 7308 96 0 R 99.7 0.0 0:22.70 stress 9834 root 20 0 1091604 293556 261636 S 0.3 1.8 0:01.06 fio 误区（2）%iowait高，磁盘IO瓶颈。由之前的解释可以看到，这个指标是描述CPU的IDLE的，它与磁盘IO是否到达瓶颈一点关系都没有，它都不是一个用来描述磁盘IO性能的指标。 实验验证1234567891011121314151617181920212223242526272829303132333435363738394041这个实验使用ssd，如下fio命令，在6个进程下，ssd的随机读性能是25k iops，%iowait超过80%。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -numjobs=6 -name=rand_100read_4krand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1...fio-3.1Starting 6 threadsJobs: 6 (f=6): [r(6)][20.0%][r=97.0MiB/s,w=0KiB/s][r=25.1k,w=0 IOPS][eta 02m:24s] top - 22:45:45 up 3:26, 3 users, load average: 4.68, 2.59, 1.81Tasks: 158 total, 1 running, 157 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.7 us, 6.7 sy, 0.0 ni, 3.4 id, 89.2 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 1.3 us, 6.4 sy, 0.0 ni, 1.3 id, 91.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 1.3 us, 7.0 sy, 0.0 ni, 3.7 id, 88.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 0.7 us, 7.0 sy, 0.0 ni, 2.0 id, 90.3 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 : 1.3 us, 6.7 sy, 0.0 ni, 1.3 id, 90.6 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 1.7 us, 7.3 sy, 0.0 ni, 5.2 id, 83.3 wa, 0.0 hi, 2.6 si, 0.0 stKiB Mem : 15934084 total, 11223276 free, 278992 used, 4431816 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14953420 avail Mem 还是这块盘，ioengine我们从psync改成aio，同时把iodepth从1改成了8，可以看到44k的IOPS，但是%iowait却是0。这里说明，%iowait大小与磁盘瓶颈毫无关系，由于aio是异步的，CPU不需要IDLE等待IO，也就不会产生%iowait。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 8 -thread -rw=randread -ioengine=libaio -bs=4k -runtime=180 -group_reporting -numjobs=6 -name=rand_100read_4krand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=8...fio-3.1Starting 6 threadsJobs: 6 (f=6): [r(6)][5.6%][r=173MiB/s,w=0KiB/s][r=44.2k,w=0 IOPS][eta 02m:50s]top - 22:49:31 up 3:30, 3 users, load average: 1.14, 2.61, 2.06Tasks: 156 total, 1 running, 155 sleeping, 0 stopped, 0 zombie%Cpu0 : 3.0 us, 4.0 sy, 0.0 ni, 93.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 2.0 us, 3.0 sy, 0.0 ni, 95.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 0.3 us, 0.7 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 5.8 us, 6.9 sy, 0.0 ni, 87.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 15934084 total, 11219948 free, 282280 used, 4431856 buff/cacheKiB Swap: 8061948 total, 8061948 free, 0 used. 14950088 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 10091 root 20 0 1431564 281112 261664 S 33.6 1.8 0:26.54 fio await 正确：表示IO从创建到完成的平均时间，包含IO排队时间+存储设备处理时间。 误区：（1）混淆%iowait和await。（2）认为await高等于磁盘慢。 解释await是iostat里的指标，iostat属于sysstat这个rpm包。await表示平均每个IO所需要的时间，包括在队列等待的时间，也包括磁盘控制器处理本次请求的有效时间。123456789man iostatawaitThe average time (in milliseconds) for I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them.这里借用blktrace的blkparse显示的各指标点，await是从IO到达block层开始算起，包含排队时间，到最后IO完毕。Q-------&gt;G------------&gt;I---------&gt;M-------------------&gt;D-----------------------------&gt;C|-Q time-|-Insert time-||--------- merge time ------------|-merge with other IO||----------------scheduler time time-------------------|---driver,adapter,storagetime--| 误区（1）混淆%iowait和await。在工作中，我遇到过几次运维和研发人员混淆这两个概念的，弄清楚并在表述的时候注意即可。 误区（2）认为await高等于磁盘慢。await包含了队列时间，我们只需要保持磁盘压力不变的情况下，增加队列深度即可验证这个问题。 实验验证1234567891011121314151617181920212223这里，我们限制IOPS为100，先用队列深度8，可以看到await 80。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdc -direct=1 -iodepth 8 -thread -rw=randread -ioengine=libaio -bs=4k -runtime=180 -group_reporting -numjobs=1 -name=rand_100read_4k -rate_iops=100rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=8fio-3.1Starting 1 threadJobs: 1 (f=1), 0-100 IOPS: [r(1)][5.6%][r=400KiB/s,w=0KiB/s][r=100,w=0 IOPS][eta 02m:50s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 97.00 0.00 0.38 0.00 8.00 8.00 84.79 84.79 0.00 10.31 100.00sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00还是限制IOPS为100，队列深度调高到16，可以看到，IOPS不变的情况下，await增加到110左右。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdc -direct=1 -iodepth 16 -thread -rw=randread -ioengine=libaio -bs=4k -runtime=180 -group_reporting -numjobs=1 -name=rand_100read_4k -rate_iops=100rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=16fio-3.1Starting 1 threadJobs: 1 (f=1), 0-100 IOPS: [r(1)][8.3%][r=400KiB/s,w=0KiB/s][r=100,w=0 IOPS][eta 02m:45s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 101.00 0.00 0.39 0.00 8.00 11.93 109.34 109.34 0.00 9.86 99.60sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 %util 正确：iostat里的%util并不能准确反映存储设备使用率（或者称为饱和率）。 误区：%util就是存储设备硬件的使用率，达到100%时就到磁盘瓶颈了。 解释%util估算了一个使用率，但是存储设备的使用率以及是否饱和，是没有接口告诉Linux系统的。%util通过/proc/diskstats计算得到，不关心等待在队里里面IO的个数，它只关心队列中有没有IO。。整个计算方式把存储设备当做只能串行处理IO的磁盘，当使用支持并发处理IO的存储设备时，此项就不准确了。当%util是100%时，代表Linux队列里一直有IO，但不能代表硬件存储设备已经饱和。123456在sysstat网站的最新文档中也已经注明： But for devices serving requests in parallel, such as RAID arrays and modern SSDs, this number does not reflect their performance limits.文档地址：http://sebastien.godard.pagesperso-orange.fr/man_iostat.html%utilPercentage of elapsed time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%. 实验验证由于%util是以单并发的存储设备设计的，只要是支持多并发的存储设备，%util就不准确了。提升并发数，即可验证这个问题。12345678910111213141516171819202122232425同样的测试用例，我们num-jobs从4改到8，可以看到，使用率都是100%，但是IOPS有巨大的不同。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -numjobs=4 -name=rand_100read_4k rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1...fio-3.1Starting 4 threadsJobs: 4 (f=4): [r(4)][3.3%][r=74.7MiB/s,w=0KiB/s][r=19.1k,w=0 IOPS][eta 02m:54s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 18078.00 0.00 70.62 0.00 8.00 3.57 0.20 0.20 0.00 0.06 100.00[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -numjobs=8 -name=rand_100read_4k rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1...fio-3.1Starting 8 threadsJobs: 8 (f=8): [r(8)][7.8%][r=110MiB/s,w=0KiB/s][r=28.2k,w=0 IOPS][eta 02m:46s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 28233.00 0.00 110.29 0.00 8.00 7.31 0.26 0.26 0.00 0.04 100.00 svctm 正确：iostat里的svctm并不能准确反映存储设备的处理时间。 误区：svctm就是存储设备硬件处理IO的时间。 解释从官方文档可以看到，Do not trust this field any more.实际上，iostat获取这个svctm仅是通过一个简单计算。iostat计算svctm需要使用%util，但%util在多并发IO里，本身就是不准确的，所以间接造成svctm也不准确。12svctmThe average service time (in milliseconds) for I/O requests that were issued to the device. Warning! Do not trust this field any more. This field will be removed in a future sysstat version. 实验验证对于相同的IO模型，存储设备的处理时间都应该是一样的。如果压力越大，存储设备处理时间应该越长才对。我们验证svctm是否和真实情况一样。123456789101112131415161718192021222324我们使用SSD做实验，可以看到，当从4个并发提升到32个并发时，IOPS从19k提升到44k，但是svctm却从0.05ms下降到0.02ms，明显不符合现实。[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -numjobs=4 -name=rand_100read_4k rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1...fio-3.1Starting 4 threadsJobs: 4 (f=4): [r(4)][5.0%][r=74.6MiB/s,w=0KiB/s][r=19.1k,w=0 IOPS][eta 02m:51s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 19069.00 0.00 74.49 0.00 8.00 3.90 0.20 0.20 0.00 0.05 100.00[root@MiWiFi-R1CM-srv ~]# fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -runtime=180 -group_reporting -numjobs=32 -name=rand_100read_4k rand_100read_4k: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1...fio-3.1Starting 32 threadsJobs: 32 (f=32): [r(32)][31.7%][r=172MiB/s,w=0KiB/s][r=44.1k,w=0 IOPS][eta 02m:03s]Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdc 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 44080.00 0.00 172.19 0.00 8.00 31.37 0.71 0.71 0.00 0.02 100.00 总结 %iowait并没有提供什么有效信息，因为它是描述IDLE的。 %iowait大小受CPU的负载变化而变化，这个指标如果很大，除了能说明CPU很闲在等待IO以外，并不能反映IO压力或者瓶颈。另外，aio不会使CPU产生%iowait。 iostat提供了基本的监控数据，但是分析IO瓶颈，关键是去看Linux的IO的监控指标是否打满了存储设备的真实性能，因此必须对存储设备在当前IO模型下具备的真实性能有所了解。 这些值虽然不够准确，但是在生产环境中，仍然具备参考意义：当你的业务类型和压力没有任何变化时，如果这些值偏离了基线（baseline）或者历史平均值，则你需要去排查是否哪里出了问题。 参考资料本文讲解并不深入，更多的是实验思路。建议阅读下列文章，包含原理性的解释，一并致谢：朱辉(茶水)： Linux Kernel iowait 时间的代码原理 深入理解iostat 辩证看待 iostat 深入理解磁盘IO利用率及饱和度]]></content>
      <categories>
        <category>性能分析</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>性能监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2019%2F03%2F%E4%BD%A0%E5%A5%BD%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
